---
layout:     post                    # 使用的布局（不需要改）
title:      Reinforcement Learning               # 标题 
subtitle:   遇到的一些强化学习面试问题 #副标题
date:       2018-11-7              # 时间
author:     ERAF                      # 作者
header-img: img/spring.png    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 强化学习
---
说道面试时候问道的强化学习方面的问题，大致可以从value-based和policy-based讲起；

## value-based

这也就是强化学习日常的基础了，也就是我们平时强化学习最常用的方法；其起源来自于动态规划，基于贝尔曼最优性得到的贝尔曼最优化方程，也就是说动态规划的核心就是找到最优值函数，进而推广从model-based到model-free，一系列强化学习方法都离不开先完成对于值函数的求取 进而求解最优策略；整体来看 其实也还是model-based里面策略迭代的那两步：策略评估「找到最优的值函数」、策略优化「基于值函数来找到策略」，可以说整个基于值函数的方法 都是离不开这两步的。

这里需要明确一个问题就是：**强化学习和动态规划的关系**，可以看到当model-based的时候 我们往往只是说使用的是动态规划的方法 而常说的TD MC什么的针对的还是model-free，难道model-based的时候就不是强化学习了吗？其实也还是强化学习问题，这里直接说强化学习其实不够准确 从本质来说 强化学习毕竟是用来解决序贯决策问题的方法，基于的是MDP，而动态规划或者说是最优控制也是解决序贯决策的优秀方法，当问题可以建模 也就是model-based的时候 其实问题就已经转化为最优控制的问题了，使用最优控制这样的方法其实也更为方便而且有着更为方便的方法，当然这里也看到了**需要精确的模型**，这也是控制方法的一个前提。其实也有一些结合最优控制和强化学习的方法 比如引导策略搜索（guided policy search）等等，这就又涉及策略梯度的方法了。

回归主题，值函数的方法肯定要对于值函数的求解，这里求解就涉及一个model-based和model-free的本质区别：转移概率$P(s|a)$知道与否，基于贝尔曼公式得到的值函数的公式$v_{\pi}(s_t)=\sum_{a_t}\pi(a_t\|s_t)\sum_{s_{t+1}}p(s_{t+1}\|s_t,a_t)[r_{t+1}+v_{\pi}(s_{t+1})]$ 「对于这个公式的推导就是从：强化学习基本的最大化长期回报：$\sum_{k=0}\gamma^kr_{t+k+1}$到策略价值$V$的定义$E_{s,a~\tau}[\sum_{k=0}\gamma^kr_{t+k+1}]$到$\sum_{\tau}p(\tau)\sum_{k=0}\gamma^kr_{t+k+1}$ 进而变换有$\sum_{(s_t,a_t,...)~\tau}\pi(a_t|s_t)\sum_{s_{t+1}}p(s_{t+1}|s_t,a_t)\sum_{k=0}\gamma^kr_{t+k+1}$最后消元替换就好得到上面的关于贝尔曼值函数的式子」

可以看到model-based因为有着环境模型，可以提前知道转移概率 考虑之后全部的可能性 遍历全部的马尔科夫链。所以不会出现后面model-free里面因为受限于采样率 与环境交互不足而导致的方差过大的问题「MC和PG都有着这样的问题」。 

对于model-free来说 其难题就是在于：没有了环境模型 因而无从得知转移概率，所以无从考虑全部策略的可能性 进而无从评估当前策略的好坏「获取值函数」；上面得到的那个值函数的公式所以也是无从使用的。对于这个难题的解决，MC和TD有着不同的思路：

对于MC来说，因为值函数还是被用来计算的还是期望。因而蒙特卡洛的方法就是基于经验平均来代替随机变量的期望「随机样本估计期望」；使用的公式依旧是$v_{\pi}(s)=E_{\pi}[G_t\|S_t=s]\text{ 有}G_t=R_{t+1}+\gamma R_{t+2}+...$ 因而还是需要等到一次试验结束之后 基于历史数据才能完成对于值函数的更新；「有些像有监督学习，基于历史数据 进行预测」

但显然蒙特卡洛的方法效率低下。而TD的方法 可以一方面借助于蒙特卡洛这种与环境交互进而基于经验估计，同时另一方面就像是model-based里面的接住后续值函数估计当前值函数的方法；TD的方法，可以利用连续两个时刻预测值的差值来更新模型；所使用的公式也还是$E_{\pi}[R_{t+1}+\gamma v_{\pi}(s_{t+1})]$ 不过和model-based的不同之处就只是在于 这里的后续状态不是基于转移概率推导出来的 而是基于试验得到的；

这里 其实就可以联想到强化学习中有着**预测和控制**的问题分类，针对的还是model-free，所谓的预测意思还是「基于预测出来后续的状态情况做出对于当前值函数的判断」，而model-based就不会有这样的“预测” 毕竟知道转移概率 就可以直白的知道后续状态的情况了，哪还需要通过与环境交互 来预测呢？

MC就先放在一边了，虽然也有着很多优秀的方法，比如AlphaGo里面使用蒙特卡洛树来进行枝展开选取走子；强化学习里面我们常说的也还是依托于TD里面的一些方法：SARSA和qlearning了；毕竟强化学习的一大精髓就是使用了原来动态规划的思想解决model-free的MDP问题；

### 相关问题1：蒙特卡洛、TD、动态规划的关系

**三者之间的相同点：**如果说三者的相同点 那么也只有都是用于进行值函数的描述与更新的了；

**两者之间的共同点**：蒙特卡洛和TD算法隶属于model-free，而动态规划属于model-based；TD算法和蒙特卡洛的方法 因为都是基于model-free的方法，因而对于后续状态的获知 也都是基于试验的方法；TD算法和动态规划的策略评估都能基于当前状态的下一步预测情况来得到对于当前状态的值函数的更新「参考公式」

**两者之间的不同点**：TD算法和蒙特卡洛的方法当然也有不同之处在于：TD算法不需要等到实验结束后 才能进行当前状态的值函数的计算与更新，而蒙特卡洛的方法需要试验交互 产生一整条的马尔科夫链「到最终状态」才能进行更新。TD算法和动态规划的策略评估不同之处还是就回到model-free和model-based 这一点上来了，动态规划可以凭借已知转移概率就能推断出来后续的状态情况，而TD只能借助试验才能知道了「因而 在强化学习的实验中 设置多次episode 以遍历尽量多的状态 和马尔科夫链 来降低方差」；

蒙特卡洛方法和TD方法的一个区别，因为蒙特卡洛方法进行完整的采样来获取了长期的回报值，因而在价值估计上会有着更小的偏差，但是也正因为收集了完整的信息 所以价值的方差会更大「毕竟基于试验的采样得到 和真实的分布还是有差距 不充足的交互导致的较大方差」；而TD算法与其相反 因为只考虑了前一步的回报值 其他都是基于之前的估计值 因而估计具有偏差 但方差也较小；

---

在介绍下面这个DQN的问题之前，首先需要知道的两个点就是：off-policy和on-policy的区别，以及什么是值函数近似；

首先是 off-policy和on-policy：对于两者区别的划分众所周知：就是看选取动作的策略和进行评估的策略是不是同一个策略就是了；而一般不同之处都是在评估的时候，进行对于下一个状态值函数预测的时候，qlearning选用的直接是`next_state`的最大的那个Q动作值函数，而sarsa还是依策略的和之前选取`state`时候动作`action`一样的选取`next_state`的动作`action_`进而确定Q动作值函数；举个SARSA和qlearning的例子就是：前者基于某一个策略如ε-greedy进行选取action，后面需要值函数进行更新的时候 也还是ε-greedy策略。后者则不然 选取action的策略是基于ε-greedy，更新值函数的时候依旧是选取max的那个；

如果再细致的来思考其中的交互过程：on-policy的方法就是遵从交互性 也就是常说的agent和environment类交互的结果、下一步的真实行动进行价值估计，于是更新也就是完全按照交互序列来的，这也是和上面介绍TD算法时候所说的一样 ：通过下一时刻的价值来更新前一时刻。 如果代入动态规划中的策略迭代来说，**on-policy有针对性的来获取这样一个target policy，直白的按照交互顺序进行更新值函数 获取策略 这样的节奏，一步步朝着最优值函数进行前进 进而获取最优的策略**。但 这样可能会到来问题就是：局部最优的问题。往大了说 还是探索与利用的矛盾，毕竟model-free 无法对于整个环境模型有着很好地感知 只能来自与环境的交互 基于当前的认知来选取动作；所以 在SARSA里面 常使用ε-greedy 在一定程度上解决了这个问题； 

off-policy或者说是qlearning 它并不是直接的采用交互序列数据来对于值函数进行更新，而是**选取了来自其他的策略来替换本来的交互序列，可以看做是一种探索的角度。毕竟经过这种方式 能够获得子部分的最优性，在前面累积的最优化结果的基础上进行更新与优化**，参照值迭代的话 其实有些像，都是选取最优的结果进行更新；

以上就是对于off-policy和on-policy的介绍，对于其好坏的评价如下：

### off-policy和on-policy的好与坏

on-policy优点因为是单纯的依照交互数据来进行更新，所以直接了当。但速度不一定快 毕竟试验交互时候还有样本的采样效率的问题，可以看到off-policy可以用到之前的经验，而on-policy只能从新采样，因而速度不一定快。同时劣势是不一定找到最优策略。

而off-policy优势在于更强的通用性 保证了探索性，其强大是因为它确保了数据全面性，所有行为都能覆盖。「这里所说的就是在选取$s_{next}$对应action的时候需要选取max的一个 那么需要全部action的q值 于是覆盖了全部的行为，相比之下on-policy只是单纯的依照策略选取了某个action；」但劣势也很明显 就是每次对于状态的值函数的估计 都是过高的进行估计。同时毕竟是基于采样的方式的 所以会有些状态没有被采样到，这就产生的偏差 在后面的DDQN里面 会有这方面的思考与解决；

### 表格式到函数近似

紧接着就是表格式到函数近似的表示；一般来说 网上找到的很多关于强化学习的介绍 都是直接那个gridworld格子世界来介绍，代码里面也就是建立个array，根据状态空间状态个数N_S和动作空间的动作个数N_A，建立个`N_S*N_A`的矩阵，用来放对应的值函数；这个表格对于求解帮助很大 也够直观 但显然对于大状态空间 就找不到建不了这么一个矩阵来放值函数，也不可能对于全部的状态动作值都求解出来，毕竟表格式的方法通过这样来保证任何一个状态和动作 都能得到对应的价值。

回头看看这个表格的目的是什么？本质上还是为了建立从状态到值函数 或者状态动作到值函数之间的映射，既然是映射，那么就肯定存在一个函数可以来表示这个映射关系，参考机器学习中的各类方法，我们可以建立一个模型来表示这个从状态到值函数 或者状态动作到值函数之间的映射关系，进而就转化为标准的机器学习中有监督学习的回归问题求解：选择好算法 损失函数 优化方法，进而就能完成求解，这里使用qlearning做个例子：

目标函数选用$L(\theta)=E[(TargetQ-Q(s,a;\theta))^2]$ ，至于这里式子的得到就是来自于表格式的时候 直接使用$Q^*(s,a) = Q(s,a)+\alpha(r+\gamma \max_{a'}Q(s',a')-Q(s,a))$ 来对于值函数就直接更新，函数近似后值函数本质上就是个函数的形式如$Q=f(\theta;feature)$ ，那么对于它的优化与更新也更多的是采用诸如梯度下降等方法来对于权值θ进行更新；同时这里有：$TargetQ = r+\gamma \max_{a'}Q(s',a';\theta)$ ，进而使用梯度下降法 得到梯度表达形式其实也就是$\frac{\partial L }{\partial \theta}=\sum_{i}^{N}(TargetQ-Q(s,a;\theta))\frac{\partial Q(s,a;\theta) }{\partial \theta}$ 「联系之前的表格式的方法 其实也能看到$\frac{\partial Q(s,a;\theta) }{\partial \theta}$如果为1 其实就是表格式时候的更新表达，所以说 本质上表格式的方法也就是函数近似表示的一种**梯度为1**的特例」

以上就是函数近似的一些思想和方法，把强化学习和机器学习联系了起来，吧对于策略的求解方法转化成了一个对于诸如神经网络的求解问题，之前的那些有关机器学习的积累都可以拿来使用；

### 相关问题2：DQN的几个改变

了解完上面那些积累后，其实DQN就没有什么太多的东西了，下面直接列举出来伪代码、网络模型、算法流程图，毕竟有时候还是会让你手撕一波代码的，然后再说其中的改变：

![](https://ws1.sinaimg.cn/large/005A8OOUly1fvwhxw05j4j30rr0kzdyh.jpg)

![](https://ws1.sinaimg.cn/large/005A8OOUly1fvwi4d1xkyj30s60huk23.jpg)

![](https://ws1.sinaimg.cn/large/005A8OOUly1fvwi5sut8bj30lq0e9jvo.jpg)

很显然DQN相比于qlearning主要有三处改变：

1.  首先DQN采用了深度卷积神经网络来进行的值函数逼近，这里选取卷积神经网络的原因也在于 原文是针对Atari游戏来作为environment的，输入状态采用的是`84x84`的图片，那么这里其实就涉及一个深度学习中一个选取问题就是CNN的优势 毕竟不是深度学习的部分 这里直接说结论，CNN凭借着本身稀疏连接和参数共享 相比于FCN来说 计算量大大减少，存储的参数量也大大减少；进而本身有着的局部特征和平移等变性 也让CNN很适合用于处理图片。当然尽管说这么多，这也不是什么新鲜操作了；但加入了第二点 就完全不同了，解决了RL里面的一个痛点；

2.  加入了经验回放的操作来训练强化学习；首先我们要知道 如果直接借助强化学习交互产生的数据 本身是带有关联性的，而在神经网络或者直接说机器学习中 对于数据的基本要求就是独立同分布。因而这里引入了经验回放这个操作 来打破了数据间的关联性，具体操作就是：agent在与环境交互的时候 将交互数据存放在一个库里面，然后训练的时候 从中随机采样数据进行训练；

    >   「这里的**有关联性的意思** ：单纯的从RL来说，毕竟是需要生成一条马尔科夫链，一个个状态动作对，相互转换得到的一条链，不是SL常见的那种独立采样的数据，肯定会带来关联性。再进一步的想，其实也就是决策的本质，毕竟某一个时刻所做出来的决策选取的工作 不光是基于当前的情况的，还是需要考虑前面时刻的结果；举个例子 比如自主驾驶中某一个时刻同样的场景 因为之前所做出决策的不同 可能会导致当前做出不同的选择，你是加速来到这个状态的 还是刹车来到这个状态的 对应的你需要做出不同的动作选择才是；也就是说你当前做出来的选择不仅是基于当前状态的考虑还要考虑之前的情况，这也就是决策（RL）和映射（SL）的区别所在；再具体到RL 」「进而这样的数据 可能会出现一个问题就是，可能相同的状态情况有着不同的动作选择，用机器学习的话来说 一个data对应多个label，这样的数据如何进行训练呢？当然这种问题在大状态空间中还是不需要考虑的，毕竟可能性太低，一般来说 只需要解决之前的前后关联性问题就好」

3.  加入了目标网络的概念来单独的处理TD算法中的TD偏差；首先这一点并没有在13年版的DQN里面使用，而是在15年版的DQN才出现的，这也是13年和15年版的区别。我们都知道 强化学习在表格式的时候 直接使用$Q^*(s,a) = Q(s,a)+\alpha(r+\gamma \max_{a'}Q(s',a')-Q(s,a))$ 来对于值函数就直接更新了；但函数近似后不行，现在的值函数本质上就是个函数的形式如$Q=f(\theta;feature)$ ，那么对于它的优化与更新也更多的是采用诸如梯度下降等方法来对于权值θ进行更新，有：$TargetQ = r+\gamma \max_{a'}Q(s',a';\theta)$ 和 $L(\theta)=E[(TargetQ-Q(s,a;\theta))^2]$ 。**因而目标网络的改进就在于这里的$TargetQ $使用了一个单独的网络** ；「emmm 这么一说 本来qlearning里面就不是按照交互顺序 而是使用的最大值来得到的对于当前值函数的估计，现在又进一步打破关联性 使用另一个网络的输出来进行估计 进一步减少关联性」也就是说现在有着$TargetQ = r+\gamma \max_{a'}Q(s',a';\theta^-)$ ，对于这个target 网络的权值$\theta^-$ 和前面的计算Q值的主网络不同，主网络是每一步一更新，而target网络每隔一段时间一更新；「联系后面的DDQN的话 两者的区别其实也就在于：DDQN是在选取动作的时候基于一个网络 而评价是基于另一个网络；也就是$TargetQ=R+\gamma Q(S_{t+1},argmax_aQ(S_{t+1},a;\theta_t);\theta'_t)$ 可以看到区别就是对于targetQ构建的时候 next state的action和q值选取还是不一样的」

其他的还有一些文章中的细节，比如考虑到部分可观测性，立一种基于时间关系的observation集成 进而完成对于信息的整合，于是15年的版本对于**网络层的输入，并非是单独的一张图片，而是把四张图片做一个外部缓冲区合并 然后作为输入到神经网络**「当然 文章里面对每一帧做了灰度处理后 合并输入 也就是batch∗width∗higth∗4batch∗width∗higth∗4 ，说是这样说其实更类似于抽样 每四帧抽样一次 对应的状态图片和action，但是和抽样不同的在于 把中间隔着的三帧数据也作为了状态那一帧的一部分作为状态进行输入了，当然action的话 还是状态帧的那个」当然 不代表这个方法就是好的，毕竟只是在那些简单的游戏中有着较好的表现；实际上 不一定就是正确的 存在一系列的问题 比如存储这些缓存的话需要很大的内存；同时 对于整个事件「某个需要作出决策的事件」来说 单单几帧的数据并不能进行代表；

这个问题进一步的可能会问一些关于DQN的改变，比如DDQN和dueling DQN等，前者上面也说过了 主要是在于对于值函数评估的时候使用了单独的网络；后者就是在构造网络的时候 dueling network 将后续的输出分为了两个分支，一条输出标量的关于状态的价值，另外一条输出关于动作的Advantage价值函数的值  ;  具体来说 就是：在评估Q (S,A)的时候也同时评估了跟动作无关的状态的价值函数V(S)和在状态下各个动作的相对价值函数A(S,A)的值 ；这里不再赘述；

### 相关问题3：深度强化学习中的DQN里面的experience replay方法和A3C的asynchronous

显而易见的 前者的经验重放就是一种标准的off-policy的方法，毕竟在DQN里面使用的时候是在对估计Q(s,a)，estimate_Q(s, a) = r + lamda * maxQ(s2, a) ，如果按照on-policy理解的话 这个时候的应该是estimate_Q(s, a) = r + lamda * Q(s2, a2)  也就是说这个时候应该保存当时候的网络模型来进行输出对应的Q值情况，进而进行基于相应的值函数 在基于相同的如ε-greedy来选择情况；现在经验重播中只保存了$<s,a,r,s_{next}>$ 谈何on-policy呢。

同时上面也说了是直接保存的交互数据进行训练，这样观察数据往往波动很大且前后sample相互关联「机器学习也要求样本彼此独立同分布」 经验重播的方法很不适合on-policy；

但多线程的synchronous不一样；因为是多个agent在多个环境实例中并行且异步的执行和学习。 数据就不存在上面所说的数据关联性的问题。多个并行的actor可以有助于exploration。在不同线程上使用不同的探索策略，使得经验数据在时间上的相关性很小。这样不需要DQN中的experience replay也可以起到稳定学习过程的作用，意味着学习过程可以是on-policy的。 

## policy-based

以上说了这么多 其实还是没有摆脱的最优控制中求解最优值函数进而求解得到策略的思路。在花书《深度学习》里面曾提到一个问题：优化和学习 的区别，这里的基于值函数的方法其实就是“学习”：借助优化和学习值函数，以此为评判标准来获取一个好的策略了。那么我们同样也可以直接对于策略本身来进行优化 也就是说直接在策略空间直接进行搜索，不再借助值函数那样的间接完成对于最优策略的确定；于是也带来了收敛性快这样的好处；方法包括策略梯度 DDPG等；

同时和value-based相比，value-based对于连续动作 不能对每个动作都给予一个Q值 因而在连续动作集合中就不能有很好的表现；而policy-based可以有效地处理连续动作集的问题「值函数方法无法确定一个对应max Q的action」，但同样的容易收敛到局部最小值、方差较大。「这里的方差较大的原因其实和蒙特卡洛方法类似 每次交互都会产生一整条轨迹 然后基于这个策略更新的时候 会导致回报估计的波动 因而会导致高方差」

### 相关问题4：策略梯度的推导（存在的问题）

接下来 就从似然函数的角度来推导一下策略梯度：

上面 我们说过无论是基于值函数还是基于策略搜索的方法，本质上的目的也都是强化学习的目的：让累计折扣回报最大化；对于一个马尔科夫过程 所产生的一组状态-动作序列 我们用$\tau $表示$s_0,a_0,s_1,a_1...s_H,a_H$

那么对于这么一组状态-动作对 或者说这一条轨迹的回报我们用 $R(\tau)=\sum^{H}_{t=0}R(s_t,a_t)$，而该轨迹出现的概率我们使用$P(\tau;\theta)$表示，那么显而易见的从强化学习的目标：最大化回报 这个目的出发，可以写作 $max_{\theta}U(\theta)=max_{\theta}E(\sum^{H}_{t=0}R(s_t,a_t))=max_{\theta}\sum_{\tau}P(\tau;\theta)R(\tau)$ 「从回报期望的形式 到写作与概率相乘的形式，之前似然函数的部分说过」

进而为了优化这个目标函数，常见的如GD 找到那个最快下降的方向，于是对其求导有：

$\nabla _{\theta } U( \theta ) =\nabla _{\theta }\sum\limits _{\tau } P( \tau ;\theta ) R( \tau ) $

$=\sum\limits _{\tau } \nabla _{\theta } P( \tau ;\theta ) R( \tau ) $

$=\sum\limits _{\tau }\frac{P( \tau ;\theta )}{P( \tau ;\theta )} \nabla _{\theta } P( \tau ;\theta ) R( \tau ) $

$=\sum\limits _{\tau } P( \tau ;\theta )\frac{\nabla _{\theta } P( \tau ;\theta ) R( \tau )}{P( \tau ;\theta )} $

 $=\sum\limits _{\tau } P( \tau ;\theta ) R( \tau ) \nabla _{\theta } logP( \tau ;\theta ) $

于是 梯度的计算转换为求解$ R( \tau ) \nabla _{\theta } logP( \tau ;\theta ) $的期望，此时可以利用蒙特卡洛法近似「经验平均」估算，即根据当前策略π采样得到m条轨迹 ，利用m条轨迹的经验平均逼近策略梯度 于是有：

$$\nabla _{\theta } U( \theta ) \approx \frac{1}{m}\sum\limits ^{m}_{i=0} R( \tau ) \nabla _{\theta } logP( \tau ;\theta ) $$

如此 我们就得到策略梯度的推导结果；如果直观的理解上面的公式的话，$\nabla _{\theta } logP( \tau ;\theta )$ 就是轨迹随着参数θ变化最陡的方向，而 $R( \tau )$ 代指了步长 影响了某轨迹出现的概率；

然后 参考现在的策略梯度中loss function的表示是 $logP(\tau;\theta )R(\tau)$ ；我们使用轨迹回报直接表达整个序列的价值也是根据定义来的毫无错误。但毕竟是基于model-free的，数据的产生来自于试验中的交互，导致着交互并不能很好的表示轨迹真实的期望 就和蒙特卡洛方法类似，会产生较大的方差「agent和环境交互 每一时刻可能产生很多结果 进而产生的一条马尔科夫链可以有多种可能，采样的至少其中一种，所以和所有路径的回报期望还是差距很大的」

### 相关问题5：对于策略梯度高方差的解决「actor-critic」「策略梯度和actor-critic的对比」

>   还有对于容易陷入局部最优问题的解决 使用TRPO：通过优化方法使每个局部点找到让损失函数非增的最优步长 来解决学习率的问题；

对于策略梯度这种因为采样问题导致的高方差，就像之前提到过MC和TD存在的问题一样，MC基于交互产生整条马尔科夫链而进行估计值函数，因为采样的问题 导致的高方差，而TD就只是利用连续两个时刻预测值的差值来更新模型，尽管基于后续状态对于当前状态进行估计，因此计算出来的策略梯度都存在偏差，但同样的换来较小的方差；

同样的思路：构建出来一个独立的模型来估计模型的长期误差，而不是单纯的使用轨迹的真实分布；进而产生了作为基于值函数方法和基于策略梯度方法的actor-critic方法；

actor是基于策略梯度的方法进行选取动作，而critic是基于值函数的方法来评价它，两者协作完成；

普通的策略梯度中loss function的表示是 $logP(\tau;\theta )R(\tau)$ 前者代指的是方向,进而基于θ求偏导的话 毋庸置疑也就是让轨迹τ的概率变化最快的方向，或快速增加或快速减少，只是取决于正负号；后者的是$R(\tau)$ 作为一个标量 类似于前者的增幅 当其为正的时候，当这个值越大的时候 轨迹τ出现的概率$P(\tau;\theta)$ 在参数更新之后会越大「建立的函数是两者相乘 但更新的参数只是在前者这个描述出现概率的变量中出现」反之则越来越小。所以，策略梯度的方法会增大高回报轨迹对应的出现概率 而会降低低回报轨迹对应的出现概率；

这个角度再来理解actor-critic，轨迹的回报$R(\tau)$ 可看做一个critic；用于评价参数θ更新后 该轨迹出现的概率是该增大还是减少 以及对应的幅度；对应的可以表示成：

![](https://ws1.sinaimg.cn/large/005A8OOUgy1fu4zxc8pctj30h90ev0vz.jpg)

 1--3：直接应用轨迹的回报累积回报，由此计算出来的策略梯度不存在偏差，但是由于需要累积多步的回报，因此方差会很大。

4—6: 利用动作值函数，优势函数和TD偏差代替累积回报，其优点是方差小，但是这三种方法中都用到了逼近方法，因此计算出来的策略梯度都存在偏差。这三种方法以牺牲偏差来换取小的方差。当Ψ_t取4—6时，为经典的AC方法。

于是一个广义的AC框架 就是：前面的 $\pi_{\theta}(a|s)$ 是actor作为策略函数；后面的 $\varPsi _t$ 是critic 评价函数 「换句话说 **critic使用的就是各种策略评估方法**，又回到了值函数的方法里面」

 **Actor - Critic的优点**

（总的来说就是结合两种方式优点）：

-   相比以值函数为中心的算法，Actor - Critic应用了策略梯度的做法，这能让它在连续动作或者高维动作空间中选取合适的动作, 而 Q-learning 做这件事会很困难甚至瘫痪。
-   相比单纯策略梯度，Actor - Critic应用了Q-learning或其他策略评估的做法，使得Actor Critic能进行单步更新而不是回合更新，比单纯的Policy Gradient的效率要高。最重要的还是不再使用采样得到的真实回报 降低了因为采样率导致的方差

**Actor - Critic和两者的区别**

1.  和策略梯度的区别，很明显 关于策略梯度的算法表述里面 并没有涉及关于critic部分或者具体点说就是值函数更新的部分；参考RL:an introduction 里面所描述的那样P292中所描述的那样： 策略梯度方法里 状态值函数更多的是作为一种基准而非是critic；也就是说只是作为一种基准来判断哪个状态需要被更新「for state-function」**而事实上 为了实现Policy Gradient，不管我们是计算Q，还是V，都需要一个对应的网络，这就是Critic。换句话讲，我们只有在使用Policy Gradient时完全不使用Q，仅使用reward真实值来评价，才叫做Policy Gradient，要不然Policy Gradient就需要有Q网络或者V网络，就是Actor Critic。** **「对于其中采用什么值来当做$R(\tau)$ 参考这来理解https://zhuanlan.zhihu.com/p/26882898」**
2.  和值函数的方法 差别就更大了；首先对于策略的描述 就不是值函数方法里面借助 $argmax_a Q(s,a)$ 而是使用的策略梯度的方法；当然使用对于该策略的好坏的评价 但也是一样的值函数；

然后对于一整个actor-critic的训练过程如下：

![](https://ws1.sinaimg.cn/large/005A8OOUgy1fu4zxspcrzj309w09st8t.jpg)

如图 基于策略梯度的actor基于概率来对于某状态来选取动作action；而critic基于actor的行为判别行为的得分；actor进而基于该评价值来计算出来一个td error修改选择行为的概率「换句话说就是：actor的策略梯度的方法生成得到梯度的方向「也就说之前的$logP(\tau;\theta )$」，然后进行沿着方向进行梯度的增减；我们需要一个值来判断这一个增减的方向是否正确 于是需要critic来计算出来td error」，同时基于选取的action计算TD error来更新critic；

具体到网络里面：Actor和Critic各为一个网络，Actor输入是状态输出的是动作，loss就是$log_{prob}\times td_{error}$,(和策略梯度相对应，注意到这里的loss和Policy Gradient中的差不多，只是vt换成td_error，引导奖励值vt换了来源（Critic给的）而已)，Critic输入的是状态输出的是Q值，loss是square((r+gamma*Q_next) - Q_eval)也就是square(td_error)，也就是说这里更新critic对应Q-learning是一样的均方误差。 

网络的交互也和上图一致：agent每次状态1从actor中得到一个动作a1 和env类交互得到s2和即时奖励r。然后把s1s2 r输入critic网络，更新其中的参数ω并计算得到td_error；然后把a1，s1，td_error输入到actor网络更新其中的参数θ；「TD_error信号同时指导actor网络critic网络的更新 」

---

### A3C

#### 异步（asynchronous）

A3C的全名是Asynchronous Advantage Actor-Critic ，顾名思义 是异步的架构；并非像DQN那样仅单智能体与单环境交互，A3C可以产生多交互过程；如上图可知 A3C包括一个主网络 和多个工作的有着各自参数的agent，同时的和各自的环境进行交互；相比于单个的agent 这种方法更有效之处在于 因为来自各自独立的环境 于是采集得到的经验也是独立的 于是采集得到的经验也更多元化；

#### actor critic

说完了异步（asynchronous） 然后说到actor-critic，相比于单纯的值迭代方法如：qlearning 和 策略迭代方法如：策略梯度的方法；AC有这两者的优势；在本实践之中，本网络可以既估计值函数V(s)V(s)「某确定的的状态有多好」而且还能输出策略π(s)π(s)「动作概率输出的集合」；参考上图知道 也就是在全连接层之后的不同输出；AC方法的关键之处在于：agent采用值估计「critic」来更新策略「actor」相比于传统的策略梯度的方法更有效；

#### advantage

回头看一下关于之前策略梯度的完成，其中关于损失函数的构建是直接使用的reward的折扣累计来实现的；emmmm [关于这部分策略梯度损失函数两部分的含义可以参考这个网页 或者是那个A3C.md](https://zhuanlan.zhihu.com/p/26882898) 于是在之前的策略梯度中的网络是根据这个折扣reward得到的关于该action好与坏的评价来进行对该action的鼓励亦或是阻止；这里采用advantage来替代传统的折扣reward累计；不光是能让agent判断action是否是好 而且能让其判断比预期好多少；intuitively （直觉的） 这样可以允许算法去更针对那些预测不足的区域 具体的可以参考之前的 dueling q network的架构：其中关于 Advantage:A=Q(s,a)−V(s)Advantage:A=Q(s,a)−V(s);在A3C中并不能直接确定Q值**(?)** 于是这里采用折扣reward来作为Q(s,a)的估计 来生成关于advantage的估计：A=R−V(s)A=R−V(s)同时 本教程之中又进行了修改 使用的是另一种有着更低方差的版本：[Generalized Advantage Estimation.](https://arxiv.org/pdf/1506.02438.pdf)

经典的A3C算法 是在actor-critic的基础上 采用了并行的结构运行，即它不在利用单个线程，而是利用多个线程。每个线程相当于一个智能体在随机探索，多个智能体共同探索，并行计算策略梯度，维持一个总的更新量。 

由经验可知道：online的RL算法「更新策略和选取策略一致」在和DNN简单结合后会不稳定。主要原因是观察数据往往波动很大且前后sample相互关联。像Neural fitted Q iteration和TRPO方法通过将经验数据batch，或者像DQN中通过experience replay memory对之随机采样，这些方法有效解决了前面所说的两个问题，但是也将算法限定在了off-policy方法中。 文章中，通过创建多个agent，在多个环境实例中并行且异步的执行和学习。于是，通过这种方式，在DNN下，解锁了一大批online/offline的RL算法（如Sarsa, AC, Q-learning） ；

>   「对于单个agent进行样本采样 获取的样本很可能就是高度相关的；而 machine learning 学习的条件是：sample 满足独立同分布的性质。在 DQN 中，我们引入了 experience replay 来克服这个难题。但是，这样子就是 offline 的了，因为你是先 sampling，然后将其存储起来，然后再 update 你的参数。  」

文章中不只是包含一种算法，而是将one-step Sarsa, one-step Q-learning, n-step Q-learning和advantage AC扩展至多线程异步架构。 可以看到几种方法的不同： **「AC是on-policy的policy搜索方法，而Q-learning是off-policy value-based方法。这也体现了该框架的通用性。 」**

简单地说，每个线程都有agent运行在环境的拷贝中，每一步生成一个参数的梯度，多个线程的这些梯度累加起来，一定步数后一起更新共享参数。 

所主要针对的还是产生多个独立环境，有多个 agent 对网络进行 asynchronous update，这样带来了样本间的相关性较低的好处，因此 A3C 中也没有采用 Experience Replay 的机制；这样 A3C 便支持 online 的训练模式了 ；

具体点说的话 就是启动了多个训练环境，同时进行采样 然后直接使用各个环境采集得到的样本进行计算梯度 训练更新相关的参数「actor中的policy gradient和critic中的值函数」

实践的话要有两套体系, 可以看作中央大脑拥有 `global net` 和他的参数, 每位玩家有一个 `global net` 的副本 `local net`, 可以定时向 `global net` 推送更新, 然后定时从 `global net` 那获取综合版的更新. 

---



在以上对于AC框架有了一定了解后，这里进一步的对其进行介绍，介绍两种对其的改进方法，这里于是就涉及两种概念：随机策略与确定性策略 以及 异步的感念；

### DPG和DDPG

强化学习基于马尔科夫过程建立 包含五种基本构成$<S,A,R,P,\Pi>$ ，而这里的π代指的就是策略，翻看之前的对于策略的体现 比如值函数里面 有使用ε-greedy随机策略，基于值函数来根据概率情况随机选取动作；一般来说有着$\pi_{\theta}(a|s)=P[a|s;\theta]$ ,具体的说 即使在相同的状态 每次采取的动作可能也不一样；

像是随机策略这样做有什么好处呢？最重要的也还是这里的随机性了，强化学习一直有着探索-利用这一本质的问题，使用随机策略本身就可以通过探索产生各类数据，进而改进当前策略。但同样的效率也低，需要采集大量的数据，在上面的策略梯度介绍中得知，其计算公式形如$E_{s`\rho^{\pi},a~\pi_{\theta}}[\nabla _{\theta } log \pi_{\theta}( a|s)Q^{\pi}(s,a) ]$  可以知道公式本身是关于状态和动作的期望，因而总归是需要与环境交互得到大量样本才能求取这一期望，参考上面的介绍可知「一般都是基于当前策略采样多条轨迹求取均值来进行表示」；

对于确定策略来说，就不再存在这个问题；确定策略顾名思义：相同策略（相同θ）下，在状态为S的时候 对应的动作是唯一确定的，即有着$a=\mu_{\theta}(s)$ ，所以上面那个公式中$E_{s`\rho^{\pi},a~\pi_{\theta}}[..]$ 不再需要考虑对于动作的积分来求取期望，所以显然带来的好处就是需要的采样数据很少 对于一些大动作空间的问题 需要采样的数据大大减少，也是很有帮助「原本需要采样多条轨迹求取均值来得到表示 现在几条就好」

但 这样就带来一个问题，就像公式表达的那样$a=\mu_{\theta}(s)$ 当初始状态一样 策略权值θ一样的时候，生成的轨迹永远都是一样的，不可能访问其他轨迹，因而就不可能体现探索这一概念；

对于这一问题的解决就是：off-policy；顾名思义 我们将行动策略和评价策略分开，前者为了保证探索效率 采用随机策略，而后者我们使用确定性策略来评价，再进一步的 毕竟是基于策略梯度的改进，其实也就是使用的actor-critic框架；「后面的DDPG里面引入了经验回放 实现的off-policy 」

基本的actor-critic「随机策略」的梯度为$E_{s`\rho^{\pi},a~\pi_{\theta}}[\nabla _{\theta } log \pi_{\theta}( a|s)Q^{\pi}(s,a) ]$ 显然也能看出来 这里其实还是同策略的表示，一方面策略梯度的方法生成策略动作 一方面critic来近似表达逼近他；进一步的异策略的随机梯度策略的表示为：$E_{s`\rho^{\pi},a~\beta}[\frac{\pi_{\theta}( a|s)}{\beta_{\theta}( a|s)}\nabla _{\theta } log \pi_{\theta}( a|s)Q^{\pi}(s,a) ]$ 可以看到这里的采样「行动」策略被单独使用了β表示，这里的比值涉及一个采样率的问题，也就是重要性权重：借由简单的比值来表达策略的情况；

进而我们可以得到确定性策略梯度为$E_{s`\rho^{\mu}}[\nabla _{\theta }\mu_{\theta}(s) \nabla _{a } Q^{\mu}(s,a)|_{a=\mu_{\theta}(s)} ]$ 可以看到：对于动作的情况 也就是策略不再需要积分 同时回报函数或者说值函数也需要对于动作求导；「行动策略或者说采样策略和评价策略一样都是μ」

>   「这里顺带说一下 为什么之前策略梯度方法里面需要出现log项，直白的看是因为变形的时候 为了好计算 而引入的；但实际上，还是因为随机策略需要重新加一层有关于策略π的期望，进而需要除以策略  数学转化成log(.)的倒数了。这个形式和交叉熵很接近，其实完全可以从概率角度去理解，有物理意义。 」

再进一步就是得到异策略确定性随机梯度了，形如：$E_{s`\rho^{\beta}}[\nabla _{\theta }\mu_{\theta}(s) \nabla _{a } Q^{\mu}(s,a)|_{a=\mu_{\theta}(s)} ]$ 可以看到 对于critic生成Q值的时候使用的是确定性策略，而状态的获取情况 使用的是采样策略；「放在后面的DDPG里面的话 这里的s的产生是基于当前的actor的情况，而后面的critic的评价与更新使用的是来自于经验池的数据 所以也正是异策略」

基于上面这个公式 ,我们便可以得到确定性策略异策略AC算法的更新过程了，以下 前面两个是利用值函数逼近的方法来更新值函数参数，第三行是利用确定性策略梯度的方法来更新策略参数：

![](https://ws1.sinaimg.cn/large/005A8OOUly1fwjaafqg32j30el040jrm.jpg)

进而是DDPG 和DPG的区别也只是在于：使用DNN来逼近值函数和确定性策略，参考上面的AC架构实现；（如果还是分不太清这里off-policy的概念，参考强化学习精要P284）这里是伪代码图：

![](https://pic4.zhimg.com/80/v2-8fe2d057ef4f4cdfeca525735afe0535_hd.jpg)

就像在DQN中讲的那样，当利用深度神经网络进行函数逼近的时候，强化学习算法常常不稳定。这是因为，对深度神经网络进行训练的时候往往假设输入的数据是独立同分布的，但强化学习的数据是顺序采集的，数据之间存在马尔科夫性，很显然这些数据并非独立同分布的。

为了打破数据之间的相关性，DQN用了两个技巧：经验回放和独立的目标网络。DDPG的算法便是将这两条技巧用到了DPG算法中。

具体来说就是针对上面DPG里面的ω和θ都使用单独的网络进行更新于是 DDPG的更新公式为「其实感觉参考上面的伪代码更好理解 毕竟里面训练critic的时候使用的经验池」：![](https://ws1.sinaimg.cn/large/005A8OOUly1fwjdbfcd7qj30nr07f0t9.jpg)



### 相关问题6：A3C和DDPG

所以谈两者关系的时候，首先是起源 都可以从PG说起，或者说都是actor-critic框架下的两种变形，不同之处更多的在于上面**问题3**中 提到的有关DQN和A3C对于数据关联性解决方法的体现；

其他的不同诸如 DDPG使用的是确定性策略进行动作选取，而A3C还是标准的随机策略，谈一下确定性策略和随机策略的好坏对比；还有就是参数更新的方式；






## 其他基本问题

详细答案 可查询：Reinforcement Learning: An Introduction    

1.  强化学习是什么？和有监督学习的异同？SL靠的是样本标签训练模型，RL依靠的是什么？

    P19；要点：强化学习的来历（试错学习）、目的（最大化累计回报）、基于的数学模型（MDP），依赖于交互数据；异同（可从预测的角度来回答）

2.  强化学习用来解决什么问题？「可能会提到多臂赌博机」

    序贯决策问题；多臂赌博机作为强化学习问题的简化 只是最大化一步的回报

3.  如果问题不满足马尔科夫性怎么办，当前时刻的状态和它之前很多状态有关；

    多个时刻状态并入考虑作为一个状态，或者使用RNN来学习其中隐含的时序信息；

4.  强化学习的损失函数

    表格式 更新时就是单纯的差值 如果将其看做梯度为1；则 $\frac{\partial L }{\partial \theta}=\sum_{i}^{N}(TargetQ-Q(s,a;\theta))\frac{\partial Q(s,a;\theta) }{\partial \theta}$ 目标函数选用$L(\theta)=E[(TargetQ-Q(s,a;\theta))^2]$ 

5.  为什么最优值函数就等同最优策略

    从贝尔曼方程的角度来谈：基于贝尔曼最优性得到的贝尔曼最优化方程；

6.  强化学习和动态规划的关系；

    参考上面的介绍；

7.  简述TD算法

    P115

8.  蒙特卡洛和时间差分的对比：MC和TD分别是无偏估计吗，为什么？MC、TD谁的方差大，为什么？ 

    P93 P115；前者无偏后者不是，前者方差大，从其值函数更新公式来谈；

9.  简述Q-Learning，写出其Q(s,a)更新公式 

    P125

10.  简述值函数逼近的想法

     P179

11.  策略梯度方法的推导

     P283

     