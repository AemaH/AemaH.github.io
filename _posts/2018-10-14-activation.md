---
layout:     post                    # 使用的布局（不需要改）
title:      DL之激活函数               # 标题 
subtitle:   ReLU和sigmoid #副标题
date:       2018-10-14              # 时间
author:     ERAF                      # 作者
header-img: img/asuka_4th.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 深度学习
---
这就是个神经网络绕不开的问题：选择什么激活函数；当然一开始的时候 首先我们要知道为什么需要选择激活函数；

## 为什么需要激活函数

说到激活函数，绕不开的一个问题就是：为什么需要激活函数；如果进一步联系的话，那么就是，神经网络加入激活函数引入隐层 放弃了训练问题的凸性是为什么？

首先就是说放弃训练问题的凸性代表着什么？凸性表示问题可以转化为一个凸优化问题，也就是说有着最优解；但同样的即使放弃了最优解，借助梯度下降优化的方法 可以找到表现足够好的局部最优值，也就是说现在不再拘泥于最优这个概念，只需要找到**表现足够好的一些函数族**即可；

同时引入激活函数，引入了非线性，也大大增强了函数的表达能力；解决一些线性模型无法解决的问题；同时引入激活函数 尽管可能无法凭借传统优化理论进行解决，但是借助启发式算法 我们可以找到表现足够好的解；

## 为什么引入非线性因素 能够加强网络的表达能力？

从神经网络的万能近似性可以得知：当神经网络有着哪怕一层非线性隐藏层，只要给予网络足够多数目的隐藏单元，都可以以任意精度的来拟合一个**从一个有限维空间到另一个有限维空间**的函数。 「花书P123」

但当没有这个激活函数的时候，无论多少层 本质上就依旧还是线性函数，「$w_1(w_2(w_3...(w_nx)))$」;每一层输出都只是上一层输入的线性组合；

## 常见的激活函数

### ReLU：整流线性单元

### ReLU初始化函数

一般选用Xavier初始化，也就是均值为0 方差为(1/输入的个数)的均匀分布；

$ReLU(z)=max(0,z)$![](https://ws1.sinaimg.cn/large/005A8OOUly1fuwkzobiuyj30d707d0st.jpg)

>   通常激活函数较好的默认选择；

在此基础上的一堆**ReLU扩展**：建立起来 $g(z;\alpha)=max(0,z)+\alpha min(0,z)$ 当α为0的时候就是标准的ReLU；

-   **绝对值整流**（absolute value rectification） 这个时候固定**α为-1**，于是整流函数就是绝对值函数 g(z)=|z| 

-   **渗漏整流线性单元**（Leaky ReLU, Maas et al., 2013） 

    **固定α为一个很小的值 比如0.01**，（于是相比于原本的ReLU 左侧的水平线有所梯度的样子；）

-   **参数化整流线性单元**（parametric ReLU, PReLU, He et al., 2015） ：将 **α 作为一个可学习的参数**

**maxout 单元** (Goodfellow et al., 2013a)

 进一步扩展了 `ReLU`，将其扩展成为一个可学习的k端函数「学习到一个多达k端的凸函数，」**具体来说就是建立一个有着k组参数的矩阵**：依据表达式:$h_i(x)=max_{j\in [1,k]}z_{i,j}$那样 选取激活值最大的那个作为下一层的神经元的激活值；

### sigmoid

`sigmoid(z)`，常记作 `σ(z)`: 其实就是之前所说的那个逻辑回归模型啦；

$\sigma (z)=\frac{1}{1+exp(-z)}$

### sigmoid初始化函数

positive_unitball 初始函数；也就是让每一个神经元输入的权值和为1；具体做法比如100个神经元，就是对于全部神经元都进行(0,1)均匀分布，然后除以这100个神经元的和；从而防止进入饱和区；

![](https://ws1.sinaimg.cn/large/005A8OOUly1fuwlrryfcoj30ek07jjrm.jpg)

进而在此基础上，有着两种变形：**tanh**和**softmax**；

前者的**双曲正切函数`tanh(z)` **的图像与 `sigmoid(z)` 大致相同，区别是**值域**为 `(-1, 1)` ，同时也有$tanh(z)=2\sigma(2z)-1=\frac{2}{1+exp(-2z)}-1=\frac{1-exp(-2z)}{1+exp(-2z)}=\frac{exp(z)-exp(-z)}{exp(z)+exp(-z)}$「一般在使用sigmoid的情况下 tanh往往有着更好的表现；」类似于

```python
z = np.array([1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0])
print(np.exp(z)/sum(np.exp(z)))
```



联想到逻辑回归中二分类和多分类的最大区别 其实就是采用的表达函数的不同，**softmax** 单元常作为网络的输出层，它很自然地表示了具有 k 个可能值的离散型随机变量的概率分布。 「就像是逻辑回归中多分类中选用的一样，类似于sigmoid 不过因为不再是0 1分布 于是前面的不是1 而要看做exp(0) 全部都是exp处理后的累加」

### 涉及问题：为什么tanh会有着更好的表现相比于sigmoid；

因为tanh的均值为0，像sigmoid这样输出均值非0的函数，会导致后面的神经网络的输入全部为正，进而导致对权值求梯度的时候 全部为正，于是反向传播的时候 要么全部向着正方向更新 要么全部向着负方向更新，大大降低了更新效率 使其难以收敛；

### sigmoid为什么会带来梯度消失的问题；

因为其本身分布的特性，当输入x大于某个值之后，其输出就稳定的保证在1附近了，函数值不再发生变化，导数进而趋于0；反向传播过程中 这个梯度就变得非常小 导致网络参数难以训练；「tanh也有着类似的问题」

进而引出下一个问题：

### 如何解决梯度消失问题

一个思路就是**不用sigmoid激活函数，改为ReLU激活函数**，其因为当x>0的时候 会保证一个常数作为梯度，因而不会出现梯度消失的问题；

另外一个方法就是：**批标准化（Batch Normalization）**：既然当输入过大的事 才会进入饱和区 那么就不让输入进入饱和区；因而**在激活层之前** 先加上一个BN层；类似于归一化数据的操作，但**不同之处在于除了归一化之外还需要复原，**引入了两个新的参数用 $\gamma^{(k)}$ 表示缩放因子，用 $\beta^{(k)}$ 表示平移距离。然后将之前的还原公式替换成：$y_i^{(k)} \leftarrow \gamma^{(k)} \hat x_i^{(k)}+\beta^{(k)}$ , 这样就把复原的这个操作 也变成了正常的类似网络参数的形式，也就是看做在激活层之前又了一个BN层来预处理，训练过程同样是由梯度下降进行更新；「**实际上 BN主要也还是为了处理feature scaling问题就是不同特征的维度不同**」

当然 因为只是将sigmoid的饱和区域移动到梯度较大的区域，并不能解决导数累乘导致的的梯度下降；**残差网络**不会因为累乘 或者说网络层的加深 而导致的梯度小时问题，毕竟其结构也注定了 反向传播 传递残差的时候，导数项会被分解成两个，有一个直接对输入的导数项不会消失；

### sigmoid和ReLU的对比

1.  **防止梯度消失**：
    -   从图像中也可以看到，当`sigmoid`函数在输入取绝对值非常大的正值或负值时会出现**饱和**现象——在图像上表现为变得很平，此时函数会对输入的微小变化不敏感——从而造成梯度消失；「也就是变化率不再发生变化」
    -   而`ReLU` 的导数始终是一个常数——负半区为 0，正半区为 1——所以不会发生梯度消失现象「这也是解决梯度消失的一种方法啊 换一种激活函数」
2.  **减缓过拟合****
    -   从`ReLU` 的图像可以看到，其在负半区的输出为 0。一旦神经元的激活值进入负半区，那么该激活值就不会产生梯度/不会被训练，造成了网络的稀疏性——**稀疏激活**；「类似于稀疏化激活单元的操作 」
    -   从而减少了参数之间的相互依赖 也是缓解过拟合问题的发生 
3.  **加速计算***
    -   `ReLU` 的求导不涉及浮点运算，所以速度更快

### 其他激活函数

#### 线性激活函数：

尽管单纯的只使用线性函数 不加激活层，神经网络的每一层就像是线性变换而得到的，于是网络层的整体也注定是线性的，这样就会失去万能近似的性质；但**只是部分网络层是纯线性的不加上激活函数还是可以的**，毕竟这样可以有效的**减少网络中的参数**

#### softmax

softmax 单元常作为网络的输出层，它很自然地表示了具有 k 个可能值的离散型随机变量的概率分布。 「就像是逻辑回归中多分类中选用的一样，类似于sigmoid 不过因为不再是0 1分布 于是前面的不是1 而要看做exp(0) 全部都是exp处理后的累加」

#### **径向基函数（radial basis function, RBF）**：

![](https://ws1.sinaimg.cn/large/005A8OOUly1fuwm5sohiwj3092026745.jpg)

显然在x接近W的时候 该函数更为活跃；于是在神经网络中很少使用 RBF 作为激活函数，因为它对大部分 x 都饱和到 0，所以很难优化。 

#### softplus

看做是ReLU的近似版、平滑版本；$g(z)=log(1+exp(z))$

![](https://ws1.sinaimg.cn/large/005A8OOUly1fuwm99tctmj30dy07fq34.jpg)

尽管看起来处处可导 且不完全饱和，看起来应该有着比ReLU更好的表现，但根据经验来说 其表现比ReLU表现更差；

#### 硬双曲正切函数（hard tanh）

$g(a)=max(-1,min(1,a))$ 它的形状和 tanh 以及整流线性单元类似，但是不同于后者，它是有界的。 
